seed: 4, algorithm: FedSGD, optimizer: SGD, momentum: 0.9, weight_decay: 0.0005, batch_size: 32, learning_rate: 0.01, model: lenet, dataset: MNIST, distribution: iid, im_iid_gamma: 0.01, tail_cls_from: 4, dirichlet_alpha: 0.2, cache_partition: False, num_workers: 0, record_time: False, gpu_idx: [2], my_exp: True, epochs: 200, predict_epochs: 5, predict_alpha: 0.8, show_sign_flip: False, other_show_3d: True, npy_file_name: 4.1.1, npy_prefix: None, num_clients: 30, num_adv: 6, attack: MinMax, defense: Mean, attack_params: {'gamma_init': 10, 'stop_threshold': 1e-05}, defense_params: None, benchmark: False, num_training_sample: 60000, num_channels: 1, num_classes: 10, mean: (0.1307,), std: (0.3081,), device: cuda:2, output: ./logs/FedSGD/MNIST_lenet/iid/MinMax_Mean/200_30_6_0.01_2025-03-01_14-48-58.txt

Started on Sat Mar  1 14:48:58 2025
Generating new indices
Doing iid partition
iid partition finished
Data partitioned
Clients and server are initialized
Starting Training...
Epoch 0  	Train Acc: 0.0938	Train loss: 0.0722	Test Acc: 0.0974	Test loss: 0.0722
Epoch 1  	Train Acc: 0.1094	Train loss: 0.0720	Test Acc: 0.0974	Test loss: 0.0722
Epoch 2  	Train Acc: 0.0917	Train loss: 0.0720	Test Acc: 0.0974	Test loss: 0.0722
Epoch 3  	Train Acc: 0.0990	Train loss: 0.0721	Test Acc: 0.0974	Test loss: 0.0721
Epoch 4  	Train Acc: 0.1052	Train loss: 0.0719	Test Acc: 0.0974	Test loss: 0.0721
Epoch 5  	Train Acc: 0.0938	Train loss: 0.0720	Test Acc: 0.0974	Test loss: 0.0721
Epoch 6  	Train Acc: 0.1010	Train loss: 0.0719	Test Acc: 0.0974	Test loss: 0.0720
Epoch 7  	Train Acc: 0.0969	Train loss: 0.0720	Test Acc: 0.0974	Test loss: 0.0720
Epoch 8  	Train Acc: 0.1083	Train loss: 0.0718	Test Acc: 0.0974	Test loss: 0.0720
Epoch 9  	Train Acc: 0.0948	Train loss: 0.0719	Test Acc: 0.0974	Test loss: 0.0719
Epoch 10 	Train Acc: 0.1010	Train loss: 0.0718	Test Acc: 0.0974	Test loss: 0.0719
Epoch 11 	Train Acc: 0.0854	Train loss: 0.0718	Test Acc: 0.0974	Test loss: 0.0718
Epoch 12 	Train Acc: 0.0958	Train loss: 0.0718	Test Acc: 0.0974	Test loss: 0.0718
Epoch 13 	Train Acc: 0.1031	Train loss: 0.0717	Test Acc: 0.0974	Test loss: 0.0718
Epoch 14 	Train Acc: 0.0927	Train loss: 0.0717	Test Acc: 0.0974	Test loss: 0.0717
Epoch 15 	Train Acc: 0.1010	Train loss: 0.0716	Test Acc: 0.0974	Test loss: 0.0717
Epoch 16 	Train Acc: 0.1146	Train loss: 0.0714	Test Acc: 0.0974	Test loss: 0.0716
Epoch 17 	Train Acc: 0.0771	Train loss: 0.0717	Test Acc: 0.0974	Test loss: 0.0716
Epoch 18 	Train Acc: 0.0865	Train loss: 0.0715	Test Acc: 0.0976	Test loss: 0.0715
Epoch 19 	Train Acc: 0.1021	Train loss: 0.0715	Test Acc: 0.0982	Test loss: 0.0715
Epoch 20 	Train Acc: 0.1021	Train loss: 0.0713	Test Acc: 0.0996	Test loss: 0.0714
Epoch 21 	Train Acc: 0.1073	Train loss: 0.0713	Test Acc: 0.1020	Test loss: 0.0714
Epoch 22 	Train Acc: 0.1125	Train loss: 0.0712	Test Acc: 0.1044	Test loss: 0.0713
Epoch 23 	Train Acc: 0.1042	Train loss: 0.0713	Test Acc: 0.1083	Test loss: 0.0713
Epoch 24 	Train Acc: 0.1167	Train loss: 0.0712	Test Acc: 0.1113	Test loss: 0.0712
Epoch 25 	Train Acc: 0.1177	Train loss: 0.0711	Test Acc: 0.1163	Test loss: 0.0711
Epoch 26 	Train Acc: 0.1156	Train loss: 0.0711	Test Acc: 0.1192	Test loss: 0.0711
Epoch 27 	Train Acc: 0.1177	Train loss: 0.0710	Test Acc: 0.1228	Test loss: 0.0710
Epoch 28 	Train Acc: 0.1187	Train loss: 0.0710	Test Acc: 0.1268	Test loss: 0.0709
Epoch 29 	Train Acc: 0.1458	Train loss: 0.0707	Test Acc: 0.1319	Test loss: 0.0708
Epoch 30 	Train Acc: 0.1365	Train loss: 0.0707	Test Acc: 0.1357	Test loss: 0.0707
Epoch 31 	Train Acc: 0.1437	Train loss: 0.0706	Test Acc: 0.1418	Test loss: 0.0706
Epoch 32 	Train Acc: 0.1271	Train loss: 0.0706	Test Acc: 0.1486	Test loss: 0.0705
Epoch 33 	Train Acc: 0.1667	Train loss: 0.0704	Test Acc: 0.1588	Test loss: 0.0704
Epoch 34 	Train Acc: 0.1688	Train loss: 0.0702	Test Acc: 0.1679	Test loss: 0.0702
Epoch 35 	Train Acc: 0.1531	Train loss: 0.0701	Test Acc: 0.1815	Test loss: 0.0701
Epoch 36 	Train Acc: 0.1698	Train loss: 0.0701	Test Acc: 0.1955	Test loss: 0.0699
Epoch 37 	Train Acc: 0.2083	Train loss: 0.0697	Test Acc: 0.2066	Test loss: 0.0697
